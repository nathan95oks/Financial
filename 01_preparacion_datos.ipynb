{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7771440",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 1: PREPARACIÓN DE DATOS Y VOCABULARIO\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "SEED = 42\n",
    "DATA_PATH = \"sent_train_clean_dedup.csv\"  # Asegúrate de que este archivo esté en la misma carpeta\n",
    "OUTPUT_DIR = \"data_processed\"\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Fijar semillas para reproducibilidad\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Leeyendo datos desde: {DATA_PATH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CARGA Y LIMPIEZA INICIAL\n",
    "# -----------------------------------------------------------------------------\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"No se encontró el archivo {DATA_PATH}. Por favor súbelo.\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Eliminar filas vacías\n",
    "df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "\n",
    "# Asegurar tipos de datos\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(f\"Total de registros válidos: {len(df)}\")\n",
    "print(\"\\nEjemplo de datos:\")\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. ANÁLISIS EXPLORATORIO (EDA)\n",
    "# -----------------------------------------------------------------------------\n",
    "conteo_clases = df['label'].value_counts().sort_index()\n",
    "labels_map = {0: 'Bajista', 1: 'Alcista', 2: 'Neutral'}\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(conteo_clases.index, conteo_clases.values, color=['#e74c3c', '#2ecc71', '#95a5a6'])\n",
    "plt.xticks([0, 1, 2], [labels_map[0], labels_map[1], labels_map[2]])\n",
    "plt.title(\"Distribución de Sentimientos en el Dataset\")\n",
    "plt.xlabel(\"Sentimiento\")\n",
    "plt.ylabel(\"Cantidad de Tweets\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Añadir etiquetas encima de las barras\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. DIVISIÓN DE DATOS (TRAIN / VAL / TEST)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Mezclamos el dataset\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "n_total = len(df)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "# El resto (aprox 15%) será Test\n",
    "\n",
    "train_df = df.iloc[:n_train].copy()\n",
    "val_df = df.iloc[n_train : n_train + n_val].copy()\n",
    "test_df = df.iloc[n_train + n_val:].copy()\n",
    "\n",
    "print(f\"\\nDivisión realizada:\")\n",
    "print(f\" - Train: {len(train_df)} ({len(train_df)/n_total:.1%})\")\n",
    "print(f\" - Val:   {len(val_df)} ({len(val_df)/n_total:.1%})\")\n",
    "print(f\" - Test:  {len(test_df)} ({len(test_df)/n_total:.1%})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. CREACIÓN DEL VOCABULARIO\n",
    "# -----------------------------------------------------------------------------\n",
    "def basic_tokenize(text: str):\n",
    "    \"\"\"Tokenizador simple: minúsculas y split por espacios.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    \"\"\"Crea vocabulario ignorando palabras poco frecuentes.\"\"\"\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(basic_tokenize(t))\n",
    "    \n",
    "    # Tokens especiales\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx = 2\n",
    "    \n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "# IMPORTANTE: El vocabulario se crea SOLO con los datos de entrenamiento\n",
    "print(\"\\nConstruyendo vocabulario...\")\n",
    "vocab = build_vocab(train_df[\"text\"].tolist(), min_freq=2)\n",
    "print(f\"Tamaño final del vocabulario: {len(vocab)} tokens\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. GUARDADO DE ARTEFACTOS\n",
    "# -----------------------------------------------------------------------------\n",
    "# Guardamos los CSV procesados\n",
    "train_df.to_csv(f\"{OUTPUT_DIR}/train.csv\", index=False)\n",
    "val_df.to_csv(f\"{OUTPUT_DIR}/val.csv\", index=False)\n",
    "test_df.to_csv(f\"{OUTPUT_DIR}/test.csv\", index=False)\n",
    "\n",
    "# Guardamos el vocabulario como pickle\n",
    "with open(f\"{OUTPUT_DIR}/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"\\n¡Proceso completado! Archivos guardados en la carpeta '{OUTPUT_DIR}/'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
