{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56ff7ed0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# NOTEBOOK 1: PREPARACIÓN DE DATOS Y VOCABULARIO\n",
                "# =============================================================================\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random\n",
                "import pickle\n",
                "import os\n",
                "import re\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.utils import resample\n",
                "\n",
                "# --- CONFIGURACIÓN ---\n",
                "SEED = 42\n",
                "DATA_PATH = \"sent_train.csv\"  # CAMBIO: Usamos el dataset crudo\n",
                "OUTPUT_DIR = \"data_processed\"\n",
                "\n",
                "# Crear carpeta de salida si no existe\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Fijar semillas para reproducibilidad\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "\n",
                "print(f\"Leeyendo datos desde: {DATA_PATH}\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 1. CARGA Y LIMPIEZA INICIAL\n",
                "# -----------------------------------------------------------------------------\n",
                "if not os.path.exists(DATA_PATH):\n",
                "    raise FileNotFoundError(f\"No se encontró el archivo {DATA_PATH}. Por favor súbelo.\")\n",
                "\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "\n",
                "# Eliminar filas vacías\n",
                "df = df.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
                "\n",
                "# Asegurar tipos de datos\n",
                "df[\"text\"] = df[\"text\"].astype(str)\n",
                "df[\"label\"] = df[\"label\"].astype(int)\n",
                "\n",
                "# --- NUEVO: LIMPIEZA DE TEXTO ---\n",
                "def clean_text(text):\n",
                "    # 1. Eliminar URLs\n",
                "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
                "    # 2. Eliminar menciones de usuario (opcional: reemplazar por <USER>)\n",
                "    text = re.sub(r'@\\w+', '', text)\n",
                "    # 3. Eliminar caracteres HTML (si los hubiera)\n",
                "    text = re.sub(r'<.*?>', '', text)\n",
                "    # 4. Eliminar espacios múltiples\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "print(\"Aplicando limpieza de texto...\")\n",
                "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
                "\n",
                "# Eliminar tweets que quedaron vacíos tras la limpieza\n",
                "df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
                "\n",
                "print(f\"Total de registros válidos: {len(df)}\")\n",
                "print(\"\\nEjemplo de datos limpios:\")\n",
                "print(df.head())\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 2. ANÁLISIS EXPLORATORIO (EDA)\n",
                "# -----------------------------------------------------------------------------\n",
                "conteo_clases = df['label'].value_counts().sort_index()\n",
                "labels_map = {0: 'Bajista', 1: 'Alcista', 2: 'Neutral'}\n",
                "\n",
                "plt.figure(figsize=(6, 4))\n",
                "bars = plt.bar(conteo_clases.index, conteo_clases.values, color=['#e74c3c', '#2ecc71', '#95a5a6'])\n",
                "plt.xticks([0, 1, 2], [labels_map[0], labels_map[1], labels_map[2]])\n",
                "plt.title(\"Distribución de Sentimientos en el Dataset\")\n",
                "plt.xlabel(\"Sentimiento\")\n",
                "plt.ylabel(\"Cantidad de Tweets\")\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Añadir etiquetas encima de las barras\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
                "             f'{int(height)}', ha='center', va='bottom')\n",
                "plt.show()\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 3. DIVISIÓN DE DATOS (TRAIN / VAL / TEST)\n",
                "# -----------------------------------------------------------------------------\n",
                "# Mezclamos el dataset\n",
                "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
                "\n",
                "n_total = len(df)\n",
                "n_train = int(0.70 * n_total)\n",
                "n_val = int(0.15 * n_total)\n",
                "# El resto (aprox 15%) será Test\n",
                "\n",
                "train_df = df.iloc[:n_train].copy()\n",
                "val_df = df.iloc[n_train : n_train + n_val].copy()\n",
                "test_df = df.iloc[n_train + n_val:].copy()\n",
                "\n",
                "print(f\"\\nDivisión realizada:\")\n",
                "print(f\" - Train: {len(train_df)} ({len(train_df)/n_total:.1%})\")\n",
                "print(f\" - Val:   {len(val_df)} ({len(val_df)/n_total:.1%})\")\n",
                "print(f\" - Test:  {len(test_df)} ({len(test_df)/n_total:.1%})\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 4. OVERSAMPLING (BALANCEO DE CLASES)\n",
                "# -----------------------------------------------------------------------------\n",
                "# Solo aplicamos oversampling al conjunto de ENTRENAMIENTO\n",
                "print(\"Distribución original en Train:\")\n",
                "print(train_df['label'].value_counts())\n",
                "\n",
                "# Separar clases\n",
                "df_0 = train_df[train_df.label == 0]\n",
                "df_1 = train_df[train_df.label == 1]\n",
                "df_2 = train_df[train_df.label == 2]\n",
                "\n",
                "# Encontrar la clase mayoritaria\n",
                "max_len = max(len(df_0), len(df_1), len(df_2))\n",
                "\n",
                "# Oversampling de clases minoritarias\n",
                "df_0_upsampled = resample(df_0, replace=True, n_samples=max_len, random_state=42)\n",
                "df_1_upsampled = resample(df_1, replace=True, n_samples=max_len, random_state=42)\n",
                "df_2_upsampled = resample(df_2, replace=True, n_samples=max_len, random_state=42)\n",
                "\n",
                "# Combinar\n",
                "train_df_balanced = pd.concat([df_0_upsampled, df_1_upsampled, df_2_upsampled])\n",
                "\n",
                "# Mezclar\n",
                "train_df = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(\"\\nDistribución balanceada en Train:\")\n",
                "print(train_df['label'].value_counts())\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 5. CREACIÓN DEL VOCABULARIO\n",
                "# -----------------------------------------------------------------------------\n",
                "def basic_tokenize(text: str):\n",
                "    \"\"\"Tokenizador simple: minúsculas y split por espacios.\"\"\"\n",
                "    return text.lower().split()\n",
                "\n",
                "def build_vocab(texts, min_freq=2):\n",
                "    \"\"\"Crea vocabulario ignorando palabras poco frecuentes.\"\"\"\n",
                "    counter = Counter()\n",
                "    for t in texts:\n",
                "        counter.update(basic_tokenize(t))\n",
                "    \n",
                "    # Tokens especiales\n",
                "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
                "    idx = 2\n",
                "    \n",
                "    for word, freq in counter.items():\n",
                "        if freq >= min_freq:\n",
                "            vocab[word] = idx\n",
                "            idx += 1\n",
                "            \n",
                "    return vocab\n",
                "\n",
                "# IMPORTANTE: El vocabulario se crea SOLO con los datos de entrenamiento\n",
                "print(\"\\nConstruyendo vocabulario...\")\n",
                "vocab = build_vocab(train_df[\"text\"].tolist(), min_freq=2)\n",
                "print(f\"Tamaño final del vocabulario: {len(vocab)} tokens\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 6. GUARDADO DE ARTEFACTOS\n",
                "# -----------------------------------------------------------------------------\n",
                "# Guardamos los CSV procesados\n",
                "train_df.to_csv(f\"{OUTPUT_DIR}/train.csv\", index=False)\n",
                "val_df.to_csv(f\"{OUTPUT_DIR}/val.csv\", index=False)\n",
                "test_df.to_csv(f\"{OUTPUT_DIR}/test.csv\", index=False)\n",
                "\n",
                "# Guardamos el vocabulario como pickle\n",
                "with open(f\"{OUTPUT_DIR}/vocab.pkl\", \"wb\") as f:\n",
                "    pickle.dump(vocab, f)\n",
                "\n",
                "print(f\"\\n¡Proceso completado! Archivos guardados en la carpeta '{OUTPUT_DIR}/'\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
