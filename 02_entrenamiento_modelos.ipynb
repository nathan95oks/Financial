{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "484ab480",
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# NOTEBOOK 2: ARQUITECTURA Y ENTRENAMIENTO (VERSIÓN COMPLETA Y CORREGIDA)\n",
                "# =============================================================================\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "import time\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# CONFIGURACIÓN GENERAL\n",
                "# -----------------------------------------------------------------------------\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "DATA_DIR = \"data_processed\"\n",
                "MODELS_DIR = \"models\"\n",
                "\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)\n",
                "\n",
                "# Hiperparámetros\n",
                "MAX_LEN = 40                 # Se modificará en Experimento 2\n",
                "EMBEDDING_DIM = 100\n",
                "HIDDEN_DIM = 128\n",
                "N_LAYERS = 2\n",
                "DROPOUT = 0.3\n",
                "BATCH_SIZE = 64\n",
                "LR = 1e-3\n",
                "EPOCHS = 12                 # Un poco más alto para estabilizar BiLSTM\n",
                "CLIP = 1.0                  # Gradient clipping\n",
                "PATIENCE = 3               # Early stopping\n",
                "\n",
                "print(f\"Usando dispositivo: {DEVICE}\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 1. CARGA DE DATOS Y VOCABULARIO\n",
                "# -----------------------------------------------------------------------------\n",
                "print(\"Cargando datos procesados...\")\n",
                "\n",
                "vocab = pickle.load(open(f\"{DATA_DIR}/vocab.pkl\", \"rb\"))\n",
                "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
                "val_df = pd.read_csv(f\"{DATA_DIR}/val.csv\")\n",
                "\n",
                "PAD_IDX = vocab[\"<PAD>\"]\n",
                "UNK_IDX = vocab[\"<UNK>\"]\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 2. DATASET Y ENCODING\n",
                "# -----------------------------------------------------------------------------\n",
                "def basic_tokenize(text):\n",
                "    return str(text).lower().split()\n",
                "\n",
                "def encode_text(text, vocab):\n",
                "    tokens = basic_tokenize(text)\n",
                "    ids = [vocab.get(tok, UNK_IDX) for tok in tokens]\n",
                "    return torch.tensor(ids, dtype=torch.long)\n",
                "\n",
                "class FinancialTweetsDataset(Dataset):\n",
                "    def __init__(self, df, vocab):\n",
                "        self.df = df\n",
                "        self.vocab = vocab\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        text = self.df.iloc[idx][\"text\"]\n",
                "        label = int(self.df.iloc[idx][\"label\"])\n",
                "        encoded = encode_text(text, self.vocab)\n",
                "        return encoded, label\n",
                "\n",
                "def collate_fn(batch):\n",
                "    \"\"\"Padding dinámico para secuencias variables.\"\"\"\n",
                "    texts, labels = zip(*batch)\n",
                "    lengths = [len(t) for t in texts]\n",
                "    max_len = min(MAX_LEN, max(lengths))\n",
                "\n",
                "    padded = []\n",
                "    for seq in texts:\n",
                "        seq = seq[:max_len]\n",
                "        if len(seq) < max_len:\n",
                "            seq = torch.cat([seq, torch.tensor([PAD_IDX] * (max_len - len(seq)))])\n",
                "        padded.append(seq)\n",
                "\n",
                "    padded = torch.stack(padded)\n",
                "    labels = torch.tensor(labels, dtype=torch.long)\n",
                "    lengths = torch.tensor([min(l, max_len) for l in lengths], dtype=torch.long)\n",
                "\n",
                "    return padded, labels, lengths\n",
                "\n",
                "train_ds = FinancialTweetsDataset(train_df, vocab)\n",
                "val_ds = FinancialTweetsDataset(val_df, vocab)\n",
                "\n",
                "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
                "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
                "\n",
                "print(\"Dataloaders listos.\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 3. ATENCIÓN + MODELOS RNN (LSTM, GRU, BiLSTM)\n",
                "# -----------------------------------------------------------------------------\n",
                "class AttentionPooling(nn.Module):\n",
                "    def __init__(self, hidden_dim):\n",
                "        super().__init__()\n",
                "        self.attn = nn.Linear(hidden_dim, 1)\n",
                "\n",
                "    def forward(self, rnn_output, mask):\n",
                "        # rnn_output: (batch, seq, hidden)\n",
                "        scores = self.attn(rnn_output).squeeze(-1)        # (batch, seq)\n",
                "        scores = scores.masked_fill(mask == 0, -1e9)      # Padding → -inf\n",
                "        attn_weights = torch.softmax(scores, dim=1)\n",
                "        context = torch.sum(rnn_output * attn_weights.unsqueeze(-1), dim=1)\n",
                "        return context\n",
                "\n",
                "class RecurrentClassifier(nn.Module):\n",
                "    def __init__(self, model_type, vocab_size, embed_dim, hidden_dim, out_dim, n_layers, dropout, pad_idx, bidirectional=False, use_attention=True):\n",
                "        super().__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
                "        self.model_type = model_type\n",
                "        self.bidirectional = bidirectional\n",
                "        self.use_attention = use_attention\n",
                "        self.n_layers = n_layers\n",
                "        self.hidden_dim = hidden_dim\n",
                "        \n",
                "        if model_type == 'lstm':\n",
                "            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
                "        else:\n",
                "            self.rnn = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
                "        \n",
                "        self.rnn_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
                "        \n",
                "        if use_attention:\n",
                "            self.attention = AttentionPooling(self.rnn_out_dim)\n",
                "        else:\n",
                "            self.attention = None\n",
                "            \n",
                "        self.fc = nn.Linear(self.rnn_out_dim, out_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.pad_idx = pad_idx\n",
                "\n",
                "    def forward(self, x, lengths=None):\n",
                "        mask = (x != self.pad_idx).float()\n",
                "        emb = self.embedding(x)\n",
                "        \n",
                "        if self.model_type == 'lstm':\n",
                "            rnn_out, (h_n, c_n) = self.rnn(emb)\n",
                "        else:\n",
                "            rnn_out, h_n = self.rnn(emb)\n",
                "            \n",
                "        if self.use_attention:\n",
                "            context = self.attention(rnn_out, mask)\n",
                "        else:\n",
                "            if self.bidirectional:\n",
                "                h_last = h_n.view(self.n_layers, 2, x.size(0), self.hidden_dim)[-1]\n",
                "                context = torch.cat([h_last[0], h_last[1]], dim=1)\n",
                "            else:\n",
                "                context = h_n[-1]\n",
                "                \n",
                "        logits = self.fc(self.dropout(context))\n",
                "        return logits\n",
                "\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 4. FUNCIÓN DE ENTRENAMIENTO Y EVALUACIÓN\n",
                "# -----------------------------------------------------------------------------\n",
                "def train_epoch(model, loader, optimizer, criterion):\n",
                "    model.train()\n",
                "    total_loss, correct, count = 0, 0, 0\n",
                "\n",
                "    for xb, yb, lengths in loader:\n",
                "        xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        logits = model(xb, lengths)\n",
                "        loss = criterion(logits, yb)\n",
                "        loss.backward()\n",
                "\n",
                "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
                "        optimizer.step()\n",
                "\n",
                "        total_loss += loss.item() * xb.size(0)\n",
                "        correct += (logits.argmax(1) == yb).sum().item()\n",
                "        count += xb.size(0)\n",
                "\n",
                "    return total_loss / count, correct / count\n",
                "\n",
                "def evaluate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss, correct, count = 0, 0, 0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for xb, yb, lengths in loader:\n",
                "            xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
                "            logits = model(xb, lengths)\n",
                "            loss = criterion(logits, yb)\n",
                "\n",
                "            total_loss += loss.item() * xb.size(0)\n",
                "            correct += (logits.argmax(1) == yb).sum().item()\n",
                "            count += xb.size(0)\n",
                "\n",
                "    return total_loss / count, correct / count\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 5. PÉRDIDA PONDERADA (DESBALANCE REAL)\n",
                "# -----------------------------------------------------------------------------\n",
                "counts = train_df[\"label\"].value_counts().sort_index()\n",
                "weights = 1.0 / counts\n",
                "weights = weights / weights.sum()\n",
                "class_weights = torch.tensor(weights.values, dtype=torch.float).to(DEVICE)\n",
                "\n",
                "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
                "\n",
                "print(\"Pesos por clase:\", weights.values)\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# 6. ENTRENAMIENTO DE MODELOS (LSTM, GRU, BiLSTM)\n",
                "# -----------------------------------------------------------------------------\n",
                "histories = {}\n",
                "\n",
                "configs = [\n",
                "    {'name': 'lstm_base', 'type': 'lstm', 'bi': False, 'attn': False},\n",
                "    {'name': 'lstm_attn', 'type': 'lstm', 'bi': False, 'attn': True},\n",
                "    {'name': 'gru_base',  'type': 'gru',  'bi': False, 'attn': False},\n",
                "    {'name': 'gru_attn',  'type': 'gru',  'bi': False, 'attn': True},\n",
                "    {'name': 'lstm_bi',   'type': 'lstm', 'bi': True,  'attn': False},\n",
                "    {'name': 'lstm_bi_attn','type':'lstm','bi': True,  'attn': True}\n",
                "]\n",
                "\n",
                "for conf in configs:\n",
                "    m_name = conf['name']\n",
                "    m_type = conf['type']\n",
                "    bi = conf['bi']\n",
                "    attn = conf['attn']\n",
                "    \n",
                "    print(f\"\\n{'='*40}\")\n",
                "    print(f\" Entrenando: {m_name.upper()} (Bi={bi}, Attn={attn})\")\n",
                "    print(f\"{'='*40}\")\n",
                "    \n",
                "    model = RecurrentClassifier(\n",
                "        model_type=m_type,\n",
                "        vocab_size=len(vocab),\n",
                "        embed_dim=EMBEDDING_DIM,\n",
                "        hidden_dim=HIDDEN_DIM,\n",
                "        out_dim=3,\n",
                "        n_layers=N_LAYERS,\n",
                "        dropout=DROPOUT,\n",
                "        pad_idx=vocab[\"<PAD>\"],\n",
                "        bidirectional=bi,\n",
                "        use_attention=attn\n",
                "    ).to(DEVICE)\n",
                "    \n",
                "    save_path = f\"{MODELS_DIR}/{m_name}_best_model.pth\"\n",
                "    \n",
                "    if os.path.exists(save_path):\n",
                "        print(f\"Modelo encontrado en {save_path}, cargando pesos...\")\n",
                "        checkpoint = torch.load(save_path, map_location=DEVICE)\n",
                "        model.load_state_dict(checkpoint['model_state'])\n",
                "        \n",
                "        # Evaluamos para confirmar accuracy\n",
                "        vl, va = evaluate(model, val_loader, criterion)\n",
                "        print(f\"Modelo cargado | Val Loss: {vl:.4f} Acc: {va:.4f}\")\n",
                "        \n",
                "        # Si ya existe, intentamos recuperar el tiempo si es posible, o lo dejamos en 0\n",
                "        # Para Notebook 4, es mejor re-entrenar si queremos el tiempo real.\n",
                "        # Pero aquí asumiremos que el usuario quiere re-entrenar para obtener los tiempos.\n",
                "        # Si no borramos los modelos, no se re-entrenará.\n",
                "        # POR AHORA: Si existe, no tenemos el tiempo. \n",
                "        # Opción: Forzar re-entrenamiento o advertir.\n",
                "        # Vamos a permitir que continúe, pero history tendrá tiempo 0 si no se re-entrena.\n",
                "        history = {'val_acc': [va], 'val_loss': [vl], 'total_time': 0}\n",
                "        histories[m_name] = history\n",
                "        \n",
                "    else:\n",
                "        optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
                "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
                "        \n",
                "        best_acc = 0.0\n",
                "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
                "        \n",
                "        start_time = time.time()\n",
                "        \n",
                "        for ep in range(EPOCHS):\n",
                "            tl, ta = train_epoch(model, train_loader, optimizer, criterion)\n",
                "            vl, va = evaluate(model, val_loader, criterion)\n",
                "            scheduler.step()\n",
                "            \n",
                "            history['train_loss'].append(tl)\n",
                "            history['val_loss'].append(vl)\n",
                "            history['train_acc'].append(ta)\n",
                "            history['val_acc'].append(va)\n",
                "            \n",
                "            print(f\"Epoch {ep+1}/{EPOCHS} | Train Loss: {tl:.4f} Acc: {ta:.4f} | Val Loss: {vl:.4f} Acc: {va:.4f}\")\n",
                "            \n",
                "            if va > best_acc:\n",
                "                best_acc = va\n",
                "                state = {\n",
                "                    'model_state': model.state_dict(),\n",
                "                    'config': {\n",
                "                        'model_type': m_type,\n",
                "                        'vocab_size': len(vocab),\n",
                "                        'embed_dim': EMBEDDING_DIM, \n",
                "                        'hidden_dim': HIDDEN_DIM,\n",
                "                        'n_layers': N_LAYERS,\n",
                "                        'dropout': DROPOUT,\n",
                "                        'pad_idx': vocab[\"<PAD>\"],\n",
                "                        'bidirectional': bi,\n",
                "                        'use_attention': attn\n",
                "                    },\n",
                "                    'vocab': vocab\n",
                "                }\n",
                "                torch.save(state, save_path)\n",
                "                print(f\" --> Nuevo récord! Modelo guardado en {save_path}\")\n",
                "        \n",
                "        end_time = time.time()\n",
                "        history['total_time'] = end_time - start_time\n",
                "        histories[m_name] = history\n",
                "\n",
                "\n",
                "# Guardar historial completo para Notebook 4\n",
                "with open(f\"{MODELS_DIR}/histories.pkl\", \"wb\") as f:\n",
                "    pickle.dump(histories, f)\n",
                "print(f\"\\nHistorial guardado en {MODELS_DIR}/histories.pkl\")\n",
                "print(\"\\n¡Proceso finalizado para LSTM, GRU y BiLSTM!\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}