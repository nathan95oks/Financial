{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484ab480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando datos procesados...\n",
      "Dataloaders listos.\n",
      "Pesos por clase: [0.50463279 0.37781801 0.11754921]\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BASE (Bi=False, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0998 Acc: 0.3228 | Val Loss: 1.0914 Acc: 0.5818\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 1.0143 Acc: 0.5982 | Val Loss: 0.9765 Acc: 0.5420\n",
      "Epoch 3/12 | Train Loss: 0.8874 Acc: 0.6809 | Val Loss: 0.9499 Acc: 0.6811\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.7685 Acc: 0.7248 | Val Loss: 0.9372 Acc: 0.6811\n",
      "Epoch 5/12 | Train Loss: 0.6704 Acc: 0.7664 | Val Loss: 0.9720 Acc: 0.6979\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.5802 Acc: 0.8124 | Val Loss: 1.0908 Acc: 0.7266\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.4779 Acc: 0.8529 | Val Loss: 1.1032 Acc: 0.7147\n",
      "Epoch 8/12 | Train Loss: 0.4200 Acc: 0.8770 | Val Loss: 1.1994 Acc: 0.7266\n",
      "Epoch 9/12 | Train Loss: 0.3763 Acc: 0.8921 | Val Loss: 1.1234 Acc: 0.7070\n",
      "Epoch 10/12 | Train Loss: 0.3291 Acc: 0.9088 | Val Loss: 1.2017 Acc: 0.7147\n",
      "Epoch 11/12 | Train Loss: 0.2998 Acc: 0.9155 | Val Loss: 1.2654 Acc: 0.7322\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 12/12 | Train Loss: 0.2739 Acc: 0.9234 | Val Loss: 1.3711 Acc: 0.7343\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_ATTN (Bi=False, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0092 Acc: 0.6105 | Val Loss: 0.9232 Acc: 0.6538\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.8055 Acc: 0.7058 | Val Loss: 0.8460 Acc: 0.7007\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.6400 Acc: 0.7700 | Val Loss: 0.8144 Acc: 0.7105\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.4566 Acc: 0.8400 | Val Loss: 0.8691 Acc: 0.7224\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.3699 Acc: 0.8743 | Val Loss: 0.9149 Acc: 0.7105\n",
      "Epoch 6/12 | Train Loss: 0.3048 Acc: 0.8924 | Val Loss: 1.0097 Acc: 0.7392\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.2295 Acc: 0.9204 | Val Loss: 1.1266 Acc: 0.7483\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.1883 Acc: 0.9332 | Val Loss: 1.1968 Acc: 0.7524\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 9/12 | Train Loss: 0.1640 Acc: 0.9440 | Val Loss: 1.2346 Acc: 0.7497\n",
      "Epoch 10/12 | Train Loss: 0.1315 Acc: 0.9537 | Val Loss: 1.3310 Acc: 0.7545\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 11/12 | Train Loss: 0.1152 Acc: 0.9581 | Val Loss: 1.3896 Acc: 0.7469\n",
      "Epoch 12/12 | Train Loss: 0.1026 Acc: 0.9642 | Val Loss: 1.4442 Acc: 0.7545\n",
      "\n",
      "========================================\n",
      " Entrenando: GRU_BASE (Bi=False, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0944 Acc: 0.3996 | Val Loss: 1.0822 Acc: 0.5769\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.9878 Acc: 0.5991 | Val Loss: 0.9904 Acc: 0.6552\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.8384 Acc: 0.6902 | Val Loss: 0.9167 Acc: 0.5755\n",
      "Epoch 4/12 | Train Loss: 0.6657 Acc: 0.7570 | Val Loss: 0.9246 Acc: 0.6909\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.5387 Acc: 0.8183 | Val Loss: 0.9560 Acc: 0.6853\n",
      "Epoch 6/12 | Train Loss: 0.4363 Acc: 0.8598 | Val Loss: 1.0213 Acc: 0.6965\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.3426 Acc: 0.8911 | Val Loss: 1.0786 Acc: 0.7196\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.2855 Acc: 0.9146 | Val Loss: 1.1261 Acc: 0.7357\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 9/12 | Train Loss: 0.2464 Acc: 0.9254 | Val Loss: 1.2953 Acc: 0.7315\n",
      "Epoch 10/12 | Train Loss: 0.2028 Acc: 0.9386 | Val Loss: 1.3062 Acc: 0.7399\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 11/12 | Train Loss: 0.1781 Acc: 0.9464 | Val Loss: 1.3177 Acc: 0.7420\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 12/12 | Train Loss: 0.1598 Acc: 0.9524 | Val Loss: 1.3905 Acc: 0.7441\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "\n",
      "========================================\n",
      " Entrenando: GRU_ATTN (Bi=False, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0209 Acc: 0.5552 | Val Loss: 0.9634 Acc: 0.6063\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.8145 Acc: 0.6629 | Val Loss: 0.8536 Acc: 0.6399\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.6399 Acc: 0.7476 | Val Loss: 0.8287 Acc: 0.7140\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.4644 Acc: 0.8223 | Val Loss: 0.8864 Acc: 0.7336\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.3762 Acc: 0.8578 | Val Loss: 0.9513 Acc: 0.7126\n",
      "Epoch 6/12 | Train Loss: 0.2945 Acc: 0.8854 | Val Loss: 1.0157 Acc: 0.7322\n",
      "Epoch 7/12 | Train Loss: 0.2181 Acc: 0.9212 | Val Loss: 1.0907 Acc: 0.7336\n",
      "Epoch 8/12 | Train Loss: 0.1761 Acc: 0.9336 | Val Loss: 1.2622 Acc: 0.7573\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 9/12 | Train Loss: 0.1466 Acc: 0.9494 | Val Loss: 1.2978 Acc: 0.7503\n",
      "Epoch 10/12 | Train Loss: 0.1136 Acc: 0.9587 | Val Loss: 1.3397 Acc: 0.7531\n",
      "Epoch 11/12 | Train Loss: 0.1015 Acc: 0.9642 | Val Loss: 1.5069 Acc: 0.7629\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 12/12 | Train Loss: 0.0925 Acc: 0.9681 | Val Loss: 1.5527 Acc: 0.7622\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BI (Bi=True, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0086 Acc: 0.5850 | Val Loss: 0.9035 Acc: 0.6867\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.7887 Acc: 0.7017 | Val Loss: 0.8316 Acc: 0.6713\n",
      "Epoch 3/12 | Train Loss: 0.5946 Acc: 0.7843 | Val Loss: 0.7830 Acc: 0.6811\n",
      "Epoch 4/12 | Train Loss: 0.4107 Acc: 0.8518 | Val Loss: 0.8866 Acc: 0.7301\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.3006 Acc: 0.8935 | Val Loss: 0.9676 Acc: 0.7517\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.2274 Acc: 0.9194 | Val Loss: 1.1574 Acc: 0.7587\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.1548 Acc: 0.9425 | Val Loss: 1.2143 Acc: 0.7657\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.1193 Acc: 0.9557 | Val Loss: 1.2887 Acc: 0.7503\n",
      "Epoch 9/12 | Train Loss: 0.0967 Acc: 0.9664 | Val Loss: 1.5133 Acc: 0.7734\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 10/12 | Train Loss: 0.0650 Acc: 0.9760 | Val Loss: 1.6280 Acc: 0.7671\n",
      "Epoch 11/12 | Train Loss: 0.0584 Acc: 0.9799 | Val Loss: 1.7265 Acc: 0.7692\n",
      "Epoch 12/12 | Train Loss: 0.0499 Acc: 0.9817 | Val Loss: 1.7557 Acc: 0.7643\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BI_ATTN (Bi=True, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 0.9910 Acc: 0.5664 | Val Loss: 0.9204 Acc: 0.6587\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.7573 Acc: 0.7059 | Val Loss: 0.8460 Acc: 0.7042\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.5566 Acc: 0.7778 | Val Loss: 0.8374 Acc: 0.7182\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.3653 Acc: 0.8656 | Val Loss: 1.0188 Acc: 0.7483\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.2542 Acc: 0.9031 | Val Loss: 1.1615 Acc: 0.7636\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.1878 Acc: 0.9309 | Val Loss: 1.3197 Acc: 0.7524\n",
      "Epoch 7/12 | Train Loss: 0.1222 Acc: 0.9507 | Val Loss: 1.4082 Acc: 0.7664\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.0853 Acc: 0.9637 | Val Loss: 1.8050 Acc: 0.7755\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 9/12 | Train Loss: 0.0666 Acc: 0.9745 | Val Loss: 1.7701 Acc: 0.7650\n",
      "Epoch 10/12 | Train Loss: 0.0411 Acc: 0.9856 | Val Loss: 1.9041 Acc: 0.7706\n",
      "Epoch 11/12 | Train Loss: 0.0315 Acc: 0.9879 | Val Loss: 2.1448 Acc: 0.7790\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 12/12 | Train Loss: 0.0269 Acc: 0.9900 | Val Loss: 2.2258 Acc: 0.7783\n",
      "\n",
      "¡Entrenamiento finalizado para LSTM, GRU y BiLSTM!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 2: ARQUITECTURA Y ENTRENAMIENTO (VERSIÓN COMPLETA Y CORREGIDA)\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# -----------------------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"data_processed\"\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Hiperparámetros\n",
    "MAX_LEN = 40                 # Se modificará en Experimento 2\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 12                 # Un poco más alto para estabilizar BiLSTM\n",
    "CLIP = 1.0                  # Gradient clipping\n",
    "PATIENCE = 3               # Early stopping\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CARGA DE DATOS Y VOCABULARIO\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Cargando datos procesados...\")\n",
    "\n",
    "vocab = pickle.load(open(f\"{DATA_DIR}/vocab.pkl\", \"rb\"))\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/val.csv\")\n",
    "\n",
    "PAD_IDX = vocab[\"<PAD>\"]\n",
    "UNK_IDX = vocab[\"<UNK>\"]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. DATASET Y ENCODING\n",
    "# -----------------------------------------------------------------------------\n",
    "def basic_tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    tokens = basic_tokenize(text)\n",
    "    ids = [vocab.get(tok, UNK_IDX) for tok in tokens]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "class FinancialTweetsDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        encoded = encode_text(text, self.vocab)\n",
    "        return encoded, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Padding dinámico para secuencias variables.\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = [len(t) for t in texts]\n",
    "    max_len = min(MAX_LEN, max(lengths))\n",
    "\n",
    "    padded = []\n",
    "    for seq in texts:\n",
    "        seq = seq[:max_len]\n",
    "        if len(seq) < max_len:\n",
    "            seq = torch.cat([seq, torch.tensor([PAD_IDX] * (max_len - len(seq)))])\n",
    "        padded.append(seq)\n",
    "\n",
    "    padded = torch.stack(padded)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor([min(l, max_len) for l in lengths], dtype=torch.long)\n",
    "\n",
    "    return padded, labels, lengths\n",
    "\n",
    "train_ds = FinancialTweetsDataset(train_df, vocab)\n",
    "val_ds = FinancialTweetsDataset(val_df, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Dataloaders listos.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. ATENCIÓN + MODELOS RNN (LSTM, GRU, BiLSTM)\n",
    "# -----------------------------------------------------------------------------\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, rnn_output, mask):\n",
    "        # rnn_output: (batch, seq, hidden)\n",
    "        scores = self.attn(rnn_output).squeeze(-1)        # (batch, seq)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)      # Padding → -inf\n",
    "        attn_weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(rnn_output * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, model_type, vocab_size, embed_dim, hidden_dim, out_dim, n_layers, dropout, pad_idx, bidirectional=False, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.model_type = model_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        \n",
    "        self.rnn_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention = AttentionPooling(self.rnn_out_dim)\n",
    "        else:\n",
    "            self.attention = None\n",
    "            \n",
    "        self.fc = nn.Linear(self.rnn_out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        mask = (x != self.pad_idx).float()\n",
    "        emb = self.embedding(x)\n",
    "        \n",
    "        if self.model_type == 'lstm':\n",
    "            rnn_out, (h_n, c_n) = self.rnn(emb)\n",
    "        else:\n",
    "            rnn_out, h_n = self.rnn(emb)\n",
    "            \n",
    "        if self.use_attention:\n",
    "            context = self.attention(rnn_out, mask)\n",
    "        else:\n",
    "            if self.bidirectional:\n",
    "                h_last = h_n.view(self.n_layers, 2, x.size(0), self.hidden_dim)[-1]\n",
    "                context = torch.cat([h_last[0], h_last[1]], dim=1)\n",
    "            else:\n",
    "                context = h_n[-1]\n",
    "                \n",
    "        logits = self.fc(self.dropout(context))\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. FUNCIÓN DE ENTRENAMIENTO Y EVALUACIÓN\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "\n",
    "    for xb, yb, lengths in loader:\n",
    "        xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, lengths)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        count += xb.size(0)\n",
    "\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, lengths in loader:\n",
    "            xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
    "            logits = model(xb, lengths)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "            count += xb.size(0)\n",
    "\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. PÉRDIDA PONDERADA (DESBALANCE REAL)\n",
    "# -----------------------------------------------------------------------------\n",
    "counts = train_df[\"label\"].value_counts().sort_index()\n",
    "weights = 1.0 / counts\n",
    "weights = weights / weights.sum()\n",
    "class_weights = torch.tensor(weights.values, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(\"Pesos por clase:\", weights.values)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. ENTRENAMIENTO DE MODELOS (LSTM, GRU, BiLSTM)\n",
    "# -----------------------------------------------------------------------------\n",
    "histories = {}\n",
    "\n",
    "configs = [\n",
    "    {'name': 'lstm_base', 'type': 'lstm', 'bi': False, 'attn': False},\n",
    "    {'name': 'lstm_attn', 'type': 'lstm', 'bi': False, 'attn': True},\n",
    "    {'name': 'gru_base',  'type': 'gru',  'bi': False, 'attn': False},\n",
    "    {'name': 'gru_attn',  'type': 'gru',  'bi': False, 'attn': True},\n",
    "    {'name': 'lstm_bi',   'type': 'lstm', 'bi': True,  'attn': False},\n",
    "    {'name': 'lstm_bi_attn','type':'lstm','bi': True,  'attn': True}\n",
    "]\n",
    "\n",
    "for conf in configs:\n",
    "    m_name = conf['name']\n",
    "    m_type = conf['type']\n",
    "    bi = conf['bi']\n",
    "    attn = conf['attn']\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\" Entrenando: {m_name.upper()} (Bi={bi}, Attn={attn})\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    model = RecurrentClassifier(\n",
    "        model_type=m_type,\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        out_dim=3,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocab[\"<PAD>\"],\n",
    "        bidirectional=bi,\n",
    "        use_attention=attn\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for ep in range(EPOCHS):\n",
    "        tl, ta = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl, va = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(tl)\n",
    "        history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(ta)\n",
    "        history['val_acc'].append(va)\n",
    "        \n",
    "        print(f\"Epoch {ep+1}/{EPOCHS} | Train Loss: {tl:.4f} Acc: {ta:.4f} | Val Loss: {vl:.4f} Acc: {va:.4f}\")\n",
    "        \n",
    "        if va > best_acc:\n",
    "            best_acc = va\n",
    "            state = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'config': {\n",
    "                    'model_type': m_type,\n",
    "                    'vocab_size': len(vocab),\n",
    "                    'embed_dim': EMBEDDING_DIM, \n",
    "                    'hidden_dim': HIDDEN_DIM,\n",
    "                    'n_layers': N_LAYERS,\n",
    "                    'dropout': DROPOUT,\n",
    "                    'pad_idx': vocab[\"<PAD>\"],\n",
    "                    'bidirectional': bi,\n",
    "                    'use_attention': attn\n",
    "                },\n",
    "                'vocab': vocab\n",
    "            }\n",
    "            save_path = f\"{MODELS_DIR}/{m_name}_best_model.pth\"\n",
    "            torch.save(state, save_path)\n",
    "            print(f\" --> Nuevo récord! Modelo guardado en {save_path}\")\n",
    "            \n",
    "    histories[m_name] = history\n",
    "\n",
    "\n",
    "print(\"\\n¡Entrenamiento finalizado para LSTM, GRU y BiLSTM!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mx110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
