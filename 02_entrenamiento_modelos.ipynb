{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484ab480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Cargando datos procesados...\n",
      "Dataloaders listos.\n",
      "Pesos por clase: [0.50463279 0.37781801 0.11754921]\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BASE (Bi=False, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0856 Acc: 0.4165 | Val Loss: 1.0715 Acc: 0.5958\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.9693 Acc: 0.6325 | Val Loss: 0.9731 Acc: 0.6580\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.8476 Acc: 0.6840 | Val Loss: 1.0869 Acc: 0.7091\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.7135 Acc: 0.7658 | Val Loss: 0.9305 Acc: 0.6846\n",
      "Epoch 5/12 | Train Loss: 0.6143 Acc: 0.8064 | Val Loss: 0.8833 Acc: 0.6972\n",
      "Epoch 6/12 | Train Loss: 0.5251 Acc: 0.8396 | Val Loss: 1.0344 Acc: 0.7084\n",
      "Epoch 7/12 | Train Loss: 0.4357 Acc: 0.8691 | Val Loss: 1.0388 Acc: 0.7203\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.3864 Acc: 0.8888 | Val Loss: 0.9945 Acc: 0.6923\n",
      "Epoch 9/12 | Train Loss: 0.3478 Acc: 0.8990 | Val Loss: 1.0386 Acc: 0.7077\n",
      "Epoch 10/12 | Train Loss: 0.3046 Acc: 0.9118 | Val Loss: 1.1379 Acc: 0.7322\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_base_best_model.pth\n",
      "Epoch 11/12 | Train Loss: 0.2830 Acc: 0.9184 | Val Loss: 1.1367 Acc: 0.7133\n",
      "Epoch 12/12 | Train Loss: 0.2611 Acc: 0.9263 | Val Loss: 1.1883 Acc: 0.7210\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_ATTN (Bi=False, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0291 Acc: 0.5386 | Val Loss: 0.9449 Acc: 0.6580\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.8350 Acc: 0.6833 | Val Loss: 0.8804 Acc: 0.6944\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.6557 Acc: 0.7554 | Val Loss: 0.8663 Acc: 0.6979\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.4651 Acc: 0.8330 | Val Loss: 0.9339 Acc: 0.6951\n",
      "Epoch 5/12 | Train Loss: 0.3678 Acc: 0.8683 | Val Loss: 0.9836 Acc: 0.7406\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.2917 Acc: 0.8966 | Val Loss: 1.0736 Acc: 0.7007\n",
      "Epoch 7/12 | Train Loss: 0.2125 Acc: 0.9275 | Val Loss: 1.1619 Acc: 0.7497\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.1710 Acc: 0.9420 | Val Loss: 1.2239 Acc: 0.7517\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 9/12 | Train Loss: 0.1377 Acc: 0.9552 | Val Loss: 1.4046 Acc: 0.7587\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_attn_best_model.pth\n",
      "Epoch 10/12 | Train Loss: 0.1035 Acc: 0.9655 | Val Loss: 1.5283 Acc: 0.7580\n",
      "Epoch 11/12 | Train Loss: 0.0938 Acc: 0.9721 | Val Loss: 1.5487 Acc: 0.7566\n",
      "Epoch 12/12 | Train Loss: 0.0832 Acc: 0.9744 | Val Loss: 1.6376 Acc: 0.7462\n",
      "\n",
      "========================================\n",
      " Entrenando: GRU_BASE (Bi=False, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0900 Acc: 0.4315 | Val Loss: 1.0622 Acc: 0.6692\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.9708 Acc: 0.6205 | Val Loss: 0.9529 Acc: 0.6392\n",
      "Epoch 3/12 | Train Loss: 0.8510 Acc: 0.6807 | Val Loss: 0.8843 Acc: 0.6042\n",
      "Epoch 4/12 | Train Loss: 0.6660 Acc: 0.7693 | Val Loss: 0.9334 Acc: 0.7035\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.5464 Acc: 0.8235 | Val Loss: 0.8817 Acc: 0.6972\n",
      "Epoch 6/12 | Train Loss: 0.4375 Acc: 0.8598 | Val Loss: 0.9669 Acc: 0.7287\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.3537 Acc: 0.8881 | Val Loss: 1.0060 Acc: 0.7420\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.3033 Acc: 0.9050 | Val Loss: 1.0145 Acc: 0.7280\n",
      "Epoch 9/12 | Train Loss: 0.2590 Acc: 0.9170 | Val Loss: 1.1508 Acc: 0.7420\n",
      "Epoch 10/12 | Train Loss: 0.2161 Acc: 0.9329 | Val Loss: 1.1653 Acc: 0.7531\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "Epoch 11/12 | Train Loss: 0.2024 Acc: 0.9387 | Val Loss: 1.1921 Acc: 0.7503\n",
      "Epoch 12/12 | Train Loss: 0.1810 Acc: 0.9413 | Val Loss: 1.2583 Acc: 0.7545\n",
      " --> Nuevo récord! Modelo guardado en models/gru_base_best_model.pth\n",
      "\n",
      "========================================\n",
      " Entrenando: GRU_ATTN (Bi=False, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0077 Acc: 0.5613 | Val Loss: 0.9257 Acc: 0.6692\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.8120 Acc: 0.6831 | Val Loss: 0.8652 Acc: 0.7070\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.6417 Acc: 0.7542 | Val Loss: 0.8227 Acc: 0.6972\n",
      "Epoch 4/12 | Train Loss: 0.4571 Acc: 0.8270 | Val Loss: 0.8744 Acc: 0.7105\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.3702 Acc: 0.8562 | Val Loss: 0.9214 Acc: 0.7503\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.2926 Acc: 0.8903 | Val Loss: 1.0098 Acc: 0.7594\n",
      " --> Nuevo récord! Modelo guardado en models/gru_attn_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.2180 Acc: 0.9184 | Val Loss: 1.0987 Acc: 0.7441\n",
      "Epoch 8/12 | Train Loss: 0.1821 Acc: 0.9335 | Val Loss: 1.1776 Acc: 0.7517\n",
      "Epoch 9/12 | Train Loss: 0.1537 Acc: 0.9425 | Val Loss: 1.2883 Acc: 0.7552\n",
      "Epoch 10/12 | Train Loss: 0.1199 Acc: 0.9591 | Val Loss: 1.3474 Acc: 0.7483\n",
      "Epoch 11/12 | Train Loss: 0.1042 Acc: 0.9637 | Val Loss: 1.4049 Acc: 0.7538\n",
      "Epoch 12/12 | Train Loss: 0.0947 Acc: 0.9670 | Val Loss: 1.4554 Acc: 0.7531\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BI (Bi=True, Attn=False)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 1.0121 Acc: 0.5751 | Val Loss: 0.9213 Acc: 0.6804\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.7955 Acc: 0.7110 | Val Loss: 0.8162 Acc: 0.7056\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.6025 Acc: 0.7850 | Val Loss: 0.8075 Acc: 0.7196\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.4197 Acc: 0.8526 | Val Loss: 0.8598 Acc: 0.7336\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.3233 Acc: 0.8893 | Val Loss: 0.9506 Acc: 0.7392\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 6/12 | Train Loss: 0.2410 Acc: 0.9140 | Val Loss: 1.1085 Acc: 0.7622\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.1643 Acc: 0.9426 | Val Loss: 1.3152 Acc: 0.7643\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.1319 Acc: 0.9521 | Val Loss: 1.3038 Acc: 0.7552\n",
      "Epoch 9/12 | Train Loss: 0.1024 Acc: 0.9628 | Val Loss: 1.5518 Acc: 0.7615\n",
      "Epoch 10/12 | Train Loss: 0.0742 Acc: 0.9745 | Val Loss: 1.5347 Acc: 0.7524\n",
      "Epoch 11/12 | Train Loss: 0.0616 Acc: 0.9777 | Val Loss: 1.5798 Acc: 0.7497\n",
      "Epoch 12/12 | Train Loss: 0.0512 Acc: 0.9819 | Val Loss: 1.8858 Acc: 0.7643\n",
      "\n",
      "========================================\n",
      " Entrenando: LSTM_BI_ATTN (Bi=True, Attn=True)\n",
      "========================================\n",
      "Epoch 1/12 | Train Loss: 0.9835 Acc: 0.5924 | Val Loss: 0.9064 Acc: 0.6895\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 2/12 | Train Loss: 0.7544 Acc: 0.7050 | Val Loss: 0.8134 Acc: 0.7035\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 3/12 | Train Loss: 0.5528 Acc: 0.7892 | Val Loss: 0.8178 Acc: 0.7105\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 4/12 | Train Loss: 0.3660 Acc: 0.8608 | Val Loss: 0.9047 Acc: 0.7469\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 5/12 | Train Loss: 0.2511 Acc: 0.9025 | Val Loss: 1.0080 Acc: 0.7455\n",
      "Epoch 6/12 | Train Loss: 0.1729 Acc: 0.9342 | Val Loss: 1.1297 Acc: 0.7517\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 7/12 | Train Loss: 0.1084 Acc: 0.9645 | Val Loss: 1.4752 Acc: 0.7678\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 8/12 | Train Loss: 0.0779 Acc: 0.9726 | Val Loss: 1.6296 Acc: 0.7566\n",
      "Epoch 9/12 | Train Loss: 0.0583 Acc: 0.9804 | Val Loss: 1.6688 Acc: 0.7706\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 10/12 | Train Loss: 0.0392 Acc: 0.9853 | Val Loss: 1.7993 Acc: 0.7664\n",
      "Epoch 11/12 | Train Loss: 0.0294 Acc: 0.9894 | Val Loss: 2.0291 Acc: 0.7741\n",
      " --> Nuevo récord! Modelo guardado en models/lstm_bi_attn_best_model.pth\n",
      "Epoch 12/12 | Train Loss: 0.0237 Acc: 0.9924 | Val Loss: 2.0418 Acc: 0.7720\n",
      "\n",
      "¡Entrenamiento finalizado para LSTM, GRU y BiLSTM!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTEBOOK 2: ARQUITECTURA Y ENTRENAMIENTO (VERSIÓN COMPLETA Y CORREGIDA)\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURACIÓN GENERAL\n",
    "# -----------------------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"data_processed\"\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Hiperparámetros\n",
    "MAX_LEN = 40                 # Se modificará en Experimento 2\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "EPOCHS = 12                 # Un poco más alto para estabilizar BiLSTM\n",
    "CLIP = 1.0                  # Gradient clipping\n",
    "PATIENCE = 3               # Early stopping\n",
    "\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. CARGA DE DATOS Y VOCABULARIO\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Cargando datos procesados...\")\n",
    "\n",
    "vocab = pickle.load(open(f\"{DATA_DIR}/vocab.pkl\", \"rb\"))\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/val.csv\")\n",
    "\n",
    "PAD_IDX = vocab[\"<PAD>\"]\n",
    "UNK_IDX = vocab[\"<UNK>\"]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. DATASET Y ENCODING\n",
    "# -----------------------------------------------------------------------------\n",
    "def basic_tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "def encode_text(text, vocab):\n",
    "    tokens = basic_tokenize(text)\n",
    "    ids = [vocab.get(tok, UNK_IDX) for tok in tokens]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "class FinancialTweetsDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        encoded = encode_text(text, self.vocab)\n",
    "        return encoded, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Padding dinámico para secuencias variables.\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = [len(t) for t in texts]\n",
    "    max_len = min(MAX_LEN, max(lengths))\n",
    "\n",
    "    padded = []\n",
    "    for seq in texts:\n",
    "        seq = seq[:max_len]\n",
    "        if len(seq) < max_len:\n",
    "            seq = torch.cat([seq, torch.tensor([PAD_IDX] * (max_len - len(seq)))])\n",
    "        padded.append(seq)\n",
    "\n",
    "    padded = torch.stack(padded)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    lengths = torch.tensor([min(l, max_len) for l in lengths], dtype=torch.long)\n",
    "\n",
    "    return padded, labels, lengths\n",
    "\n",
    "train_ds = FinancialTweetsDataset(train_df, vocab)\n",
    "val_ds = FinancialTweetsDataset(val_df, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Dataloaders listos.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. ATENCIÓN + MODELOS RNN (LSTM, GRU, BiLSTM)\n",
    "# -----------------------------------------------------------------------------\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, rnn_output, mask):\n",
    "        # rnn_output: (batch, seq, hidden)\n",
    "        scores = self.attn(rnn_output).squeeze(-1)        # (batch, seq)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)      # Padding → -inf\n",
    "        attn_weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(rnn_output * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(self, model_type, vocab_size, embed_dim, hidden_dim, out_dim, n_layers, dropout, pad_idx, bidirectional=False, use_attention=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.model_type = model_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n",
    "        \n",
    "        self.rnn_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        if use_attention:\n",
    "            self.attention = AttentionPooling(self.rnn_out_dim)\n",
    "        else:\n",
    "            self.attention = None\n",
    "            \n",
    "        self.fc = nn.Linear(self.rnn_out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        mask = (x != self.pad_idx).float()\n",
    "        emb = self.embedding(x)\n",
    "        \n",
    "        if self.model_type == 'lstm':\n",
    "            rnn_out, (h_n, c_n) = self.rnn(emb)\n",
    "        else:\n",
    "            rnn_out, h_n = self.rnn(emb)\n",
    "            \n",
    "        if self.use_attention:\n",
    "            context = self.attention(rnn_out, mask)\n",
    "        else:\n",
    "            if self.bidirectional:\n",
    "                h_last = h_n.view(self.n_layers, 2, x.size(0), self.hidden_dim)[-1]\n",
    "                context = torch.cat([h_last[0], h_last[1]], dim=1)\n",
    "            else:\n",
    "                context = h_n[-1]\n",
    "                \n",
    "        logits = self.fc(self.dropout(context))\n",
    "        return logits\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. FUNCIÓN DE ENTRENAMIENTO Y EVALUACIÓN\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "\n",
    "    for xb, yb, lengths in loader:\n",
    "        xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, lengths)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        count += xb.size(0)\n",
    "\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, count = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, lengths in loader:\n",
    "            xb, yb, lengths = xb.to(DEVICE), yb.to(DEVICE), lengths.to(DEVICE)\n",
    "            logits = model(xb, lengths)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "            count += xb.size(0)\n",
    "\n",
    "    return total_loss / count, correct / count\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. PÉRDIDA PONDERADA (DESBALANCE REAL)\n",
    "# -----------------------------------------------------------------------------\n",
    "counts = train_df[\"label\"].value_counts().sort_index()\n",
    "weights = 1.0 / counts\n",
    "weights = weights / weights.sum()\n",
    "class_weights = torch.tensor(weights.values, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(\"Pesos por clase:\", weights.values)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. ENTRENAMIENTO DE MODELOS (LSTM, GRU, BiLSTM)\n",
    "# -----------------------------------------------------------------------------\n",
    "histories = {}\n",
    "\n",
    "configs = [\n",
    "    {'name': 'lstm_base', 'type': 'lstm', 'bi': False, 'attn': False},\n",
    "    {'name': 'lstm_attn', 'type': 'lstm', 'bi': False, 'attn': True},\n",
    "    {'name': 'gru_base',  'type': 'gru',  'bi': False, 'attn': False},\n",
    "    {'name': 'gru_attn',  'type': 'gru',  'bi': False, 'attn': True},\n",
    "    {'name': 'lstm_bi',   'type': 'lstm', 'bi': True,  'attn': False},\n",
    "    {'name': 'lstm_bi_attn','type':'lstm','bi': True,  'attn': True}\n",
    "]\n",
    "\n",
    "for conf in configs:\n",
    "    m_name = conf['name']\n",
    "    m_type = conf['type']\n",
    "    bi = conf['bi']\n",
    "    attn = conf['attn']\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\" Entrenando: {m_name.upper()} (Bi={bi}, Attn={attn})\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    model = RecurrentClassifier(\n",
    "        model_type=m_type,\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        out_dim=3,\n",
    "        n_layers=N_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=vocab[\"<PAD>\"],\n",
    "        bidirectional=bi,\n",
    "        use_attention=attn\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for ep in range(EPOCHS):\n",
    "        tl, ta = train_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl, va = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(tl)\n",
    "        history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(ta)\n",
    "        history['val_acc'].append(va)\n",
    "        \n",
    "        print(f\"Epoch {ep+1}/{EPOCHS} | Train Loss: {tl:.4f} Acc: {ta:.4f} | Val Loss: {vl:.4f} Acc: {va:.4f}\")\n",
    "        \n",
    "        if va > best_acc:\n",
    "            best_acc = va\n",
    "            state = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'config': {\n",
    "                    'model_type': m_type,\n",
    "                    'vocab_size': len(vocab),\n",
    "                    'embed_dim': EMBEDDING_DIM, \n",
    "                    'hidden_dim': HIDDEN_DIM,\n",
    "                    'n_layers': N_LAYERS,\n",
    "                    'dropout': DROPOUT,\n",
    "                    'pad_idx': vocab[\"<PAD>\"],\n",
    "                    'bidirectional': bi,\n",
    "                    'use_attention': attn\n",
    "                },\n",
    "                'vocab': vocab\n",
    "            }\n",
    "            save_path = f\"{MODELS_DIR}/{m_name}_best_model.pth\"\n",
    "            torch.save(state, save_path)\n",
    "            print(f\" --> Nuevo récord! Modelo guardado en {save_path}\")\n",
    "            \n",
    "    histories[m_name] = history\n",
    "\n",
    "\n",
    "print(\"\\n¡Entrenamiento finalizado para LSTM, GRU y BiLSTM!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mx110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
